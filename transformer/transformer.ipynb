{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IQxgUxWzuho"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q5jgJrOzvLn"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\r\n",
        "    def __init__(self, embedding_size, heads):\r\n",
        "        super().__init__()\r\n",
        "        self.embedding_size = embedding_size\r\n",
        "        self.heads = heads\r\n",
        "        self.head_dim = embedding_size // heads\r\n",
        "\r\n",
        "        assert(self.heads * self.head_dim == self.embedding_size), \"Invalid number of heads\"\r\n",
        "\r\n",
        "        self.fc_values = nn.Linear(self.head_dim, self.head_dim, bias=False)\r\n",
        "        self.fc_keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\r\n",
        "        self.fc_queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\r\n",
        "\r\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embedding_size)\r\n",
        "\r\n",
        "    def forward(self, values, keys, queries, mask):\r\n",
        "        N = queries.shape[0]\r\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\r\n",
        "\r\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\r\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\r\n",
        "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\r\n",
        "\r\n",
        "        values = self.fc_values(values)\r\n",
        "        keys = self.fc_keys(keys)\r\n",
        "        queries = self.fc_queries(queries)\r\n",
        "\r\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\r\n",
        "        if mask is not None:\r\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\r\n",
        "\r\n",
        "        energy = torch.softmax(energy / (self.embedding_size ** 0.5), dim=3)\r\n",
        "        attention = torch.einsum(\"nhql,nlhd->nqhd\", [energy, values])\r\n",
        "        attention = attention.reshape(N, query_len, self.heads * self.head_dim)\r\n",
        "        out = self.fc_out(attention)\r\n",
        "\r\n",
        "        return out"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdnW6zAgzvJC"
      },
      "source": [
        "class TransformerBlock(nn.Module):\r\n",
        "    def __init__(self, embedding_size, heads, forward_expansion, p):\r\n",
        "        super().__init__()\r\n",
        "        self.attention = MultiHeadAttention(embedding_size, heads)\r\n",
        "        self.norm1 = nn.LayerNorm(embedding_size)\r\n",
        "\r\n",
        "        self.feed_forward = nn.Sequential(nn.Linear(embedding_size, forward_expansion * embedding_size),\r\n",
        "                                          nn.ReLU(),\r\n",
        "                                          nn.Linear(forward_expansion * embedding_size, embedding_size))\r\n",
        "        self.norm2 = nn.LayerNorm(embedding_size)\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(p)\r\n",
        "\r\n",
        "    def forward(self, values, keys, queries, mask):\r\n",
        "        attention_out = self.attention(values, keys, queries, mask)\r\n",
        "        x = self.norm1(attention_out + queries)\r\n",
        "        x = self.dropout(x)\r\n",
        "\r\n",
        "        ff_out = self.feed_forward(x)\r\n",
        "        out = self.norm2(ff_out + x)\r\n",
        "        out = self.dropout(out)\r\n",
        "\r\n",
        "        return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUPJ4aBpzvFy"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, src_vocab_size, embedding_size, num_layers, heads, \r\n",
        "                 forward_expansion, max_length, p, device):\r\n",
        "        super().__init__()\r\n",
        "        self.device = device\r\n",
        "\r\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embedding_size)\r\n",
        "        self.positional_embedding = nn.Embedding(max_length, embedding_size)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([TransformerBlock(embedding_size, heads, forward_expansion, p) for _ in range(num_layers)])\r\n",
        "        self.dropout = nn.Dropout(p)\r\n",
        "\r\n",
        "    def forward(self, x, mask):\r\n",
        "        N, seq_len = x.shape\r\n",
        "        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\r\n",
        "        out = self.dropout((self.word_embedding(x) + self.positional_embedding(positions)))\r\n",
        "\r\n",
        "        for layer in self.layers:\r\n",
        "            out = layer(out, out, out ,mask)\r\n",
        "\r\n",
        "        return out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnT1PFLjEpL0"
      },
      "source": [
        "class DecoderBlock(nn.Module):\r\n",
        "    def __init__(self, embedding_size, heads, forward_expansion, p, device):\r\n",
        "        super().__init__()\r\n",
        "        self.attention = MultiHeadAttention(embedding_size, heads)\r\n",
        "        self.norm = nn.LayerNorm(embedding_size)\r\n",
        "        \r\n",
        "        self.transformer_block = TransformerBlock(embedding_size, heads, forward_expansion, p)\r\n",
        "        self.dropout = nn.Dropout(p)\r\n",
        "\r\n",
        "    def forward(self, x, values, keys, src_mask, trg_mask):\r\n",
        "        attention_out = self.attention(x, x, x, trg_mask)\r\n",
        "        queries = self.dropout(self.norm(attention_out + x))\r\n",
        "        out = self.transformer_block(values, keys, queries, src_mask)\r\n",
        "\r\n",
        "        return out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh1kVlAAEpJc"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, trg_vocab_size, embedding_size, num_layers, heads,\r\n",
        "                 forward_expansion, max_length, p, device):\r\n",
        "        super().__init__()\r\n",
        "        self.device = device\r\n",
        "\r\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\r\n",
        "        self.positional_embedding = nn.Embedding(max_length, embedding_size)\r\n",
        "\r\n",
        "        self.layers = nn.ModuleList([DecoderBlock(embedding_size, heads, forward_expansion, p, device) for _ in range(num_layers)])\r\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\r\n",
        "        self.dropout = nn.Dropout(p)\r\n",
        "\r\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\r\n",
        "        N, seq_len = x.shape\r\n",
        "        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\r\n",
        "        x = self.dropout((self.word_embedding(x) + self.positional_embedding(positions)))\r\n",
        "\r\n",
        "        for layer in self.layers:\r\n",
        "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\r\n",
        "\r\n",
        "        out = self.fc_out(x)\r\n",
        "\r\n",
        "        return out"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBPbNP8vGpRK"
      },
      "source": [
        "class Transformer(nn.Module):\r\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embedding_size=512,\r\n",
        "                 num_layers=6, forward_expansion=4, heads=8, max_length=100, p=0, device=\"cpu\"):\r\n",
        "        super().__init__()\r\n",
        "        self.src_pad_idx = src_pad_idx\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "\r\n",
        "        self.encoder = Encoder(src_vocab_size, embedding_size, num_layers, heads, \r\n",
        "                               forward_expansion, max_length, p, device)\r\n",
        "        \r\n",
        "        self.decoder = Decoder(trg_vocab_size, embedding_size, num_layers, heads, \r\n",
        "                               forward_expansion, max_length, p, device)\r\n",
        "        \r\n",
        "    def get_src_mask(self, src):\r\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2).to(self.device)\r\n",
        "        return src_mask\r\n",
        "\r\n",
        "    def get_trg_mask(self, trg):\r\n",
        "        N, trg_len = trg.shape\r\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len).to(self.device)\r\n",
        "        return trg_mask\r\n",
        "\r\n",
        "    def forward(self, src, trg):\r\n",
        "        src_mask = self.get_src_mask(src)\r\n",
        "        trg_mask = self.get_trg_mask(trg)\r\n",
        "\r\n",
        "        enc_out = self.encoder(src, src_mask)\r\n",
        "        out = self.decoder(trg, enc_out, src_mask, trg_mask)\r\n",
        "\r\n",
        "        return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC77cKBtJ_kj"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "src_pad_idx = 0\r\n",
        "trg_pad_idx = 0\r\n",
        "src_vocab_size = 10\r\n",
        "trg_vocab_size = 10"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZwYOTGweMRS",
        "outputId": "8b4b3255-531f-478f-9aa0-6d3af85108eb"
      },
      "source": [
        "device"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNpTE5G5dOzp"
      },
      "source": [
        "src = torch.tensor([[1, 5, 7, 3, 4, 6, 8, 9, 2, 0, 0],\r\n",
        "                    [1, 4, 3, 3, 7, 8, 9, 5, 3, 2, 0],\r\n",
        "                    [1, 8, 9, 4, 5, 8, 3, 6, 6, 9, 2]]).to(device)\r\n",
        "\r\n",
        "trg = torch.tensor([[1, 5, 4, 3, 9, 4, 2, 0],\r\n",
        "                    [1, 6, 9, 8, 3, 6, 8, 2],\r\n",
        "                    [1, 9, 8, 7, 6, 2, 0, 0]]).to(device)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alb2BHa_dURY",
        "outputId": "f505d6ab-f3d2-4960-d114-5c868f9f6525"
      },
      "source": [
        "src.shape, trg.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 11]), torch.Size([3, 8]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_afZ3zveEUI"
      },
      "source": [
        "net = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qugUf-wPeEQf",
        "outputId": "30501266-42e7-4c08-9a55-e26c0a8c22e1"
      },
      "source": [
        "net"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (word_embedding): Embedding(10, 512)\n",
              "    (positional_embedding): Embedding(100, 512)\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (1): TransformerBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (2): TransformerBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (3): TransformerBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (4): TransformerBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (5): TransformerBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (word_embedding): Embedding(10, 512)\n",
              "    (positional_embedding): Embedding(100, 512)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): MultiHeadAttention(\n",
              "            (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (1): DecoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): MultiHeadAttention(\n",
              "            (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (2): DecoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): MultiHeadAttention(\n",
              "            (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (3): DecoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): MultiHeadAttention(\n",
              "            (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (4): DecoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): MultiHeadAttention(\n",
              "            (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (5): DecoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): MultiHeadAttention(\n",
              "            (fc_values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=512, out_features=10, bias=True)\n",
              "    (dropout): Dropout(p=0, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuuVeJ-Ve2im",
        "outputId": "2783a4b2-6931-4b7b-9249-997a0f0e278d"
      },
      "source": [
        "out = net(src, trg[:, :-1])\r\n",
        "out, out.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 1.8968e-01,  4.6380e-01,  5.1210e-01,  8.0332e-01,  4.1375e-01,\n",
              "           -1.5518e-02, -3.1026e-01, -1.0664e+00, -5.0201e-01,  1.4194e-01],\n",
              "          [-7.5107e-01, -6.0959e-03,  8.5103e-01, -2.7004e-01,  1.4141e-01,\n",
              "            6.5634e-01, -4.3093e-01, -2.4749e-01, -7.1317e-01,  6.8862e-02],\n",
              "          [-5.7888e-01, -1.1531e-01,  5.0558e-01, -4.4965e-03,  5.6342e-01,\n",
              "           -2.3258e-01,  2.5970e-02, -8.3897e-01, -3.6586e-01,  2.1711e-01],\n",
              "          [-1.6662e-01,  8.8083e-01,  7.3900e-01,  1.4125e-01, -1.2750e-01,\n",
              "            2.5745e-01, -1.1150e-01, -7.0900e-01,  1.0390e-01,  5.8816e-01],\n",
              "          [-4.2889e-01,  1.8837e-01,  8.5323e-01,  3.0196e-01,  2.9685e-01,\n",
              "           -6.1869e-02,  1.3138e-01, -1.3593e+00, -3.5308e-01, -7.7775e-01],\n",
              "          [-6.5435e-01, -2.1466e-01,  2.1622e-01, -7.4070e-01,  5.3855e-01,\n",
              "            2.5033e-03, -5.9645e-01, -8.0153e-01,  3.9680e-01,  2.5530e-02],\n",
              "          [ 1.9032e-01, -2.9666e-02,  1.3042e-01, -3.1520e-01, -5.4711e-01,\n",
              "            1.1384e+00,  1.3618e-01, -6.9797e-01,  2.9250e-01, -6.9760e-01]],\n",
              " \n",
              "         [[ 1.5929e-01,  2.8950e-01,  2.2566e-01,  1.0206e+00,  5.0757e-01,\n",
              "           -2.2427e-01, -2.1013e-01, -1.0598e+00, -2.7859e-01,  2.3298e-01],\n",
              "          [-1.4484e-02,  4.1010e-01,  1.5126e-02,  5.8926e-01, -5.9910e-02,\n",
              "           -5.5816e-01,  4.3463e-02, -9.6716e-01,  2.3389e-01, -6.9923e-01],\n",
              "          [-3.3929e-01,  4.6371e-01,  7.2973e-01,  8.7405e-01,  1.1887e-01,\n",
              "            2.6551e-02, -2.7923e-01, -1.2351e+00, -1.0926e-01, -6.3123e-01],\n",
              "          [-5.8773e-01,  1.0917e+00,  7.0033e-01,  2.0859e-01, -3.7718e-01,\n",
              "            3.0336e-01,  2.5544e-01, -7.7062e-01,  8.6288e-01,  6.3214e-01],\n",
              "          [ 2.7213e-01, -4.2907e-01, -1.2563e-01,  4.4412e-01,  1.0921e-01,\n",
              "           -3.4281e-01,  1.6882e-01, -8.0100e-01,  1.6969e-01, -1.9353e-01],\n",
              "          [-2.9663e-01,  5.2374e-01, -4.6742e-01,  2.5744e-01,  9.4221e-02,\n",
              "           -8.3292e-01, -5.9115e-01, -1.0805e+00,  9.0069e-01, -5.8624e-01],\n",
              "          [ 3.1613e-01,  3.6900e-01,  1.0042e-01,  1.9999e-01, -5.2114e-01,\n",
              "            6.2287e-01, -6.0118e-01, -4.9614e-01,  1.0565e+00, -7.9210e-02]],\n",
              " \n",
              "         [[ 1.2570e-01,  4.6633e-01,  4.4080e-01,  7.9947e-01,  4.5929e-01,\n",
              "           -4.2208e-04, -2.6306e-01, -1.0089e+00, -4.9387e-01,  2.0644e-01],\n",
              "          [-2.7168e-01,  4.0583e-01,  1.0300e+00,  3.6667e-01,  1.5859e-01,\n",
              "            3.8768e-01, -4.4796e-01, -1.0547e+00, -2.9203e-01, -5.6667e-01],\n",
              "          [-6.0839e-01,  7.4531e-01,  7.3983e-01,  1.8626e-01, -2.9052e-01,\n",
              "            2.9705e-01, -4.4743e-01, -9.0387e-01,  1.2376e-01,  4.2001e-01],\n",
              "          [-2.5032e-01,  6.8445e-01,  5.8690e-01, -5.2168e-02,  5.1899e-01,\n",
              "            3.1073e-01,  7.6992e-01,  9.7765e-02, -4.5440e-02,  4.7509e-01],\n",
              "          [-3.1282e-01,  2.4076e-01, -1.3142e-01,  3.0839e-01,  5.3166e-01,\n",
              "           -7.7985e-01,  7.0131e-01, -1.0308e+00,  3.6888e-01, -8.9843e-01],\n",
              "          [-3.9055e-01, -1.4185e-01, -1.2864e-01, -5.7741e-01, -4.9014e-01,\n",
              "            5.0907e-01, -2.8556e-01, -9.1314e-01,  1.4522e-01, -4.4075e-01],\n",
              "          [ 3.2368e-01,  1.7448e-01, -9.3202e-02, -4.2286e-01, -5.3685e-01,\n",
              "            1.1819e-01, -5.1097e-01,  3.0724e-01,  3.0801e-02,  2.6184e-01]]],\n",
              "        grad_fn=<AddBackward0>), torch.Size([3, 7, 10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ULgGHHYfSnl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a36hQ8Xzrzo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}